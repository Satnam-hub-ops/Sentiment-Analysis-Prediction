{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05db17a5",
   "metadata": {},
   "source": [
    "# Overview\n",
    "    \n",
    "This Project is based on the dataset available at Kaggle which is composed ofabout 20K tweets to train sentiment predictors.\n",
    "This Notebook is going to train a model that predict the sentiment of a text/tweet, This Notebook will guide you through the process of training the model from data prepration to data cleaning & training some Deep Learing models to predict the       result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb6bb5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7223ca36",
   "metadata": {},
   "source": [
    "# Data Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33348b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "217a6714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>4eac33d1c0</td>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>d lost</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>4f4c4fc327</td>\n",
       "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
       "      <td>, don`t force</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>f67aae2310</td>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>Yay good for both of you.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>ed167662a5</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27480</th>\n",
       "      <td>6f7127d9d7</td>\n",
       "      <td>All this flirting going on - The ATG smiles...</td>\n",
       "      <td>All this flirting going on - The ATG smiles. Y...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27481 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID                                               text  \\\n",
       "0      cb774db0d1                I`d have responded, if I were going   \n",
       "1      549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2      088c60f138                          my boss is bullying me...   \n",
       "3      9642c003ef                     what interview! leave me alone   \n",
       "4      358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "...           ...                                                ...   \n",
       "27476  4eac33d1c0   wish we could come see u on Denver  husband l...   \n",
       "27477  4f4c4fc327   I`ve wondered about rake to.  The client has ...   \n",
       "27478  f67aae2310   Yay good for both of you. Enjoy the break - y...   \n",
       "27479  ed167662a5                         But it was worth it  ****.   \n",
       "27480  6f7127d9d7     All this flirting going on - The ATG smiles...   \n",
       "\n",
       "                                           selected_text sentiment  \n",
       "0                    I`d have responded, if I were going   neutral  \n",
       "1                                               Sooo SAD  negative  \n",
       "2                                            bullying me  negative  \n",
       "3                                         leave me alone  negative  \n",
       "4                                          Sons of ****,  negative  \n",
       "...                                                  ...       ...  \n",
       "27476                                             d lost  negative  \n",
       "27477                                      , don`t force  negative  \n",
       "27478                          Yay good for both of you.  positive  \n",
       "27479                         But it was worth it  ****.  positive  \n",
       "27480  All this flirting going on - The ATG smiles. Y...   neutral  \n",
       "\n",
       "[27481 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261787d8",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fc92738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>28b57f3990</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6e0c6d75b1</td>\n",
       "      <td>2am feedings for the baby are fun when he is a...</td>\n",
       "      <td>fun</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>50e14c0bb8</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>Soooo high</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>e050245fbd</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>Both of you</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fc2cbefa9d</td>\n",
       "      <td>Journey!? Wow... u just became cooler.  hehe....</td>\n",
       "      <td>Wow... u just became cooler.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2339a9b08b</td>\n",
       "      <td>as much as i love to be hopeful, i reckon the...</td>\n",
       "      <td>as much as i love to be hopeful, i reckon the ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16fab9f95b</td>\n",
       "      <td>I really really like the song Love Story by Ta...</td>\n",
       "      <td>like</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>74a76f6e0a</td>\n",
       "      <td>My Sharpie is running DANGERously low on ink</td>\n",
       "      <td>DANGERously</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>04dd1d2e34</td>\n",
       "      <td>i want to go to music tonight but i lost my vo...</td>\n",
       "      <td>lost</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bbe3cbf620</td>\n",
       "      <td>test test from the LG enV2</td>\n",
       "      <td>test test from the LG enV2</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8a939bfb59</td>\n",
       "      <td>Uh oh, I am sunburned</td>\n",
       "      <td>Uh oh, I am sunburned</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3440297f8b</td>\n",
       "      <td>S`ok, trying to plot alternatives as we speak...</td>\n",
       "      <td>*sigh*</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>919fa93391</td>\n",
       "      <td>i`ve been sick for the past few days  and thus...</td>\n",
       "      <td>sick</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>af3fed7fc3</td>\n",
       "      <td>is back home now      gonna miss every one</td>\n",
       "      <td>onna</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>40e7becabf</td>\n",
       "      <td>Hes just not that into you</td>\n",
       "      <td>Hes just not that into you</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        textID                                               text  \\\n",
       "0   cb774db0d1                I`d have responded, if I were going   \n",
       "1   549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2   088c60f138                          my boss is bullying me...   \n",
       "3   9642c003ef                     what interview! leave me alone   \n",
       "4   358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "5   28b57f3990  http://www.dothebouncy.com/smf - some shameles...   \n",
       "6   6e0c6d75b1  2am feedings for the baby are fun when he is a...   \n",
       "7   50e14c0bb8                                         Soooo high   \n",
       "8   e050245fbd                                        Both of you   \n",
       "9   fc2cbefa9d   Journey!? Wow... u just became cooler.  hehe....   \n",
       "10  2339a9b08b   as much as i love to be hopeful, i reckon the...   \n",
       "11  16fab9f95b  I really really like the song Love Story by Ta...   \n",
       "12  74a76f6e0a       My Sharpie is running DANGERously low on ink   \n",
       "13  04dd1d2e34  i want to go to music tonight but i lost my vo...   \n",
       "14  bbe3cbf620                         test test from the LG enV2   \n",
       "15  8a939bfb59                              Uh oh, I am sunburned   \n",
       "16  3440297f8b   S`ok, trying to plot alternatives as we speak...   \n",
       "17  919fa93391  i`ve been sick for the past few days  and thus...   \n",
       "18  af3fed7fc3         is back home now      gonna miss every one   \n",
       "19  40e7becabf                         Hes just not that into you   \n",
       "\n",
       "                                        selected_text sentiment  \n",
       "0                 I`d have responded, if I were going   neutral  \n",
       "1                                            Sooo SAD  negative  \n",
       "2                                         bullying me  negative  \n",
       "3                                      leave me alone  negative  \n",
       "4                                       Sons of ****,  negative  \n",
       "5   http://www.dothebouncy.com/smf - some shameles...   neutral  \n",
       "6                                                 fun  positive  \n",
       "7                                          Soooo high   neutral  \n",
       "8                                         Both of you   neutral  \n",
       "9                        Wow... u just became cooler.  positive  \n",
       "10  as much as i love to be hopeful, i reckon the ...   neutral  \n",
       "11                                               like  positive  \n",
       "12                                        DANGERously  negative  \n",
       "13                                               lost  negative  \n",
       "14                         test test from the LG enV2   neutral  \n",
       "15                              Uh oh, I am sunburned  negative  \n",
       "16                                             *sigh*  negative  \n",
       "17                                               sick  negative  \n",
       "18                                               onna  negative  \n",
       "19                         Hes just not that into you   neutral  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d771fb2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27481"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Length of the dataset\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de9462b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neutral', 'negative', 'positive'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Is there any other different value than neutral , positive & negative?\n",
    "df['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "906a6b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>7781</td>\n",
       "      <td>7781</td>\n",
       "      <td>5861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>11118</td>\n",
       "      <td>11117</td>\n",
       "      <td>11111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>8582</td>\n",
       "      <td>8582</td>\n",
       "      <td>5537</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           textID   text  selected_text\n",
       "sentiment                              \n",
       "negative     7781   7781           5861\n",
       "neutral     11118  11117          11111\n",
       "positive     8582   8582           5537"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data is biased ?\n",
    "df.groupby('sentiment').nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89747c5a",
   "metadata": {},
   "source": [
    "Data is little bit biased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5f6cb6",
   "metadata": {},
   "source": [
    "#  Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d03ed487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>http://www.dothebouncy.com/smf - some shameles...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       selected_text sentiment\n",
       "0                I`d have responded, if I were going   neutral\n",
       "1                                           Sooo SAD  negative\n",
       "2                                        bullying me  negative\n",
       "3                                     leave me alone  negative\n",
       "4                                      Sons of ****,  negative\n",
       "5  http://www.dothebouncy.com/smf - some shameles...   neutral"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The columns we are going to use are sentiment & selected_text\n",
    "train = df[['selected_text' , 'sentiment']]\n",
    "train.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86a1e4b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Is there any null values?\n",
    "train[\"selected_text\"].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de5c2dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\AppData\\Local\\Temp\\ipykernel_17420\\3312253806.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train[\"selected_text\"].fillna(\"No Content\" , inplace= True)\n"
     ]
    }
   ],
   "source": [
    "#Filling the null value\n",
    "train[\"selected_text\"].fillna(\"No Content\" , inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d335863",
   "metadata": {},
   "source": [
    "The next step about data cleaning will be:\n",
    "\n",
    "1.Removing URLs\n",
    "2.Tokenize text\n",
    "3.Removing Emails\n",
    "4.Removing new line characters\n",
    "5.Removing distracting single quotes\n",
    "6.Removing all punctuation signs\n",
    "7.Lowercase all text\n",
    "8.Detokenize text\n",
    "9.Convert list of texts to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5244d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce5f3279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def depure_data(data):\n",
    "    #Removing URLs with a regular expression\n",
    "    url_pattern = re.compile(r'https?://\\S+|www.\\S+')\n",
    "    data = url_pattern.sub(r'' , data)\n",
    "    \n",
    "    #Removing Emails\n",
    "    data = re.sub('\\S*@\\S*\\s?' , '', data)\n",
    "    \n",
    "    #Removing new line characters\n",
    "    data = re.sub('s+' , ' ', data)\n",
    "    # Removing distracting single quotes\n",
    "    data = re.sub(\"\\'\" , \"\" , data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "299dbf93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I`d have re ponded, if I were going',\n",
       " 'Sooo SAD',\n",
       " 'bullying me',\n",
       " 'leave me alone',\n",
       " 'Son  of ****,']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = []\n",
    "#Splitting pd series to list\n",
    "data_to_list = train['selected_text'].values.tolist()\n",
    "for i in range(len(data_to_list)):\n",
    "    temp.append(depure_data(data_to_list[i]))\n",
    "list(temp[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fa132bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['have', 're', 'ponded', 'if', 'were', 'going'], ['sooo', 'sad'], ['bullying', 'me'], ['leave', 'me', 'alone'], ['son', 'of'], ['ome', 'hamele', 'plugging', 'for', 'the', 'be', 'ranger', 'forum', 'on', 'earth'], ['fun'], ['soooo', 'high'], ['both', 'of', 'you'], ['wow', 'ju', 'became', 'cooler'], ['much', 'love', 'to', 'be', 'hopeful', 'reckon', 'the', 'chance', 'are', 'minimal', 'never', 'gonna', 'get', 'my', 'cake', 'and', 'tuff'], ['like'], ['dangerou', 'ly'], ['lo'], ['te', 'te', 'from', 'the', 'lg', 'env'], ['uh', 'oh', 'am', 'unburned'], ['igh'], ['ick'], ['onna'], ['he', 'ju', 'not', 'that', 'into', 'you']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence) , deacc = True))  \n",
    "        #deacc ture will remove punctuaions\n",
    "        \n",
    "data_words = list(sent_to_words(temp))\n",
    "\n",
    "print(data_words[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70a7f664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27481"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ea7f0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(text):\n",
    "    return TreebankWordDetokenizer().detokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8d13c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['have re ponded if were going', 'sooo sad', 'bullying me', 'leave me alone', 'son of', 'ome hamele plugging for the be ranger forum on earth', 'fun', 'soooo high', 'both of you', 'wow ju became cooler']\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for i in range(len(data_words)):\n",
    "    data.append(detokenize(data_words[i]))\n",
    "    \n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1713e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "178accf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['have re ponded if were going', 'sooo sad', 'bullying me', ...,\n",
       "       'yay good for both of you', 'but it wa worth it',\n",
       "       'all thi flirting going on the atg mile yay hug'], dtype='<U133')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbced8ea",
   "metadata": {},
   "source": [
    "# Label Encoding\n",
    "\n",
    "The data is categorical so , we need to convert the sentiment labels  from Neutral , Positive & Negative to a float type so your model can understand , & we are going to acheive this by using to_categorical method from keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c88db03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66f8573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(train['sentiment'])\n",
    "y = []\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] == 'neutral':\n",
    "        y.append(0)\n",
    "    if labels[i] == 'positive':\n",
    "        y.append(1)\n",
    "    if labels[i] == 'negative':\n",
    "        y.append(2)\n",
    "        \n",
    "y = np.array(y)\n",
    "labels = tf.keras.utils.to_categorical(y , 3 , dtype='float32')\n",
    "del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b0b6666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27481"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd046de",
   "metadata": {},
   "source": [
    "# Data Sequencing & Splitting\n",
    "\n",
    "I am going to implement the keras tokenizer as well as its pad_sequences method to transform your data into 3D float data , otherwise your model is unable to trained it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c01a4efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...   72  153   46]\n",
      " [   0    0    0 ...    0 1627  490]\n",
      " [   0    0    0 ...    0    0   10]\n",
      " ...\n",
      " [   0    0    0 ...  390   11    5]\n",
      " [   0    0    0 ...   20  568    3]\n",
      " [   0    0    0 ...  437  204  464]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import regularizers\n",
    "from keras import backend as k\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "max_words = 5000\n",
    "max_len = 200\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data)\n",
    "sequenecs = tokenizer.texts_to_sequences(data)\n",
    "tweets = pad_sequences(sequenecs , maxlen=max_len)\n",
    "print(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d65947d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4161890f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20610 6871 20610 6871\n"
     ]
    }
   ],
   "source": [
    "#splitting the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train , X_test , Y_train , Y_test = train_test_split(tweets , labels , random_state = 0)\n",
    "\n",
    "print(len(X_train) ,len(X_test) , len(Y_train) , len(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f26464",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "783e3f1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.8188 - accuracy: 0.6349\n",
      "Epoch 1: val_accuracy improved from -inf to 0.74880, saving model to best_LSTM_model.hdf5\n",
      "645/645 [==============================] - 84s 122ms/step - loss: 0.8188 - accuracy: 0.6349 - val_loss: 0.6762 - val_accuracy: 0.7488\n",
      "Epoch 2/70\n",
      "  1/645 [..............................] - ETA: 1:06 - loss: 0.7320 - accuracy: 0.6875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645/645 [==============================] - ETA: 0s - loss: 0.5833 - accuracy: 0.7707\n",
      "Epoch 2: val_accuracy improved from 0.74880 to 0.79625, saving model to best_LSTM_model.hdf5\n",
      "645/645 [==============================] - 78s 121ms/step - loss: 0.5833 - accuracy: 0.7707 - val_loss: 0.5267 - val_accuracy: 0.7962\n",
      "Epoch 3/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.5084 - accuracy: 0.8076\n",
      "Epoch 3: val_accuracy improved from 0.79625 to 0.80367, saving model to best_LSTM_model.hdf5\n",
      "645/645 [==============================] - 78s 122ms/step - loss: 0.5084 - accuracy: 0.8076 - val_loss: 0.5003 - val_accuracy: 0.8037\n",
      "Epoch 4/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.4757 - accuracy: 0.8189\n",
      "Epoch 4: val_accuracy improved from 0.80367 to 0.81618, saving model to best_LSTM_model.hdf5\n",
      "645/645 [==============================] - 78s 120ms/step - loss: 0.4757 - accuracy: 0.8189 - val_loss: 0.4857 - val_accuracy: 0.8162\n",
      "Epoch 5/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.4563 - accuracy: 0.8294\n",
      "Epoch 5: val_accuracy did not improve from 0.81618\n",
      "645/645 [==============================] - 77s 119ms/step - loss: 0.4563 - accuracy: 0.8294 - val_loss: 0.4743 - val_accuracy: 0.8136\n",
      "Epoch 6/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.4407 - accuracy: 0.8346\n",
      "Epoch 6: val_accuracy improved from 0.81618 to 0.82244, saving model to best_LSTM_model.hdf5\n",
      "645/645 [==============================] - 77s 120ms/step - loss: 0.4407 - accuracy: 0.8346 - val_loss: 0.4687 - val_accuracy: 0.8224\n",
      "Epoch 7/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.4301 - accuracy: 0.8398\n",
      "Epoch 7: val_accuracy did not improve from 0.82244\n",
      "645/645 [==============================] - 77s 120ms/step - loss: 0.4301 - accuracy: 0.8398 - val_loss: 0.4699 - val_accuracy: 0.8181\n",
      "Epoch 8/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.4206 - accuracy: 0.8453\n",
      "Epoch 8: val_accuracy improved from 0.82244 to 0.82783, saving model to best_LSTM_model.hdf5\n",
      "645/645 [==============================] - 78s 120ms/step - loss: 0.4206 - accuracy: 0.8453 - val_loss: 0.4587 - val_accuracy: 0.8278\n",
      "Epoch 9/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.4107 - accuracy: 0.8503\n",
      "Epoch 9: val_accuracy did not improve from 0.82783\n",
      "645/645 [==============================] - 77s 120ms/step - loss: 0.4107 - accuracy: 0.8503 - val_loss: 0.4578 - val_accuracy: 0.8272\n",
      "Epoch 10/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.4048 - accuracy: 0.8515\n",
      "Epoch 10: val_accuracy improved from 0.82783 to 0.82928, saving model to best_LSTM_model.hdf5\n",
      "645/645 [==============================] - 78s 120ms/step - loss: 0.4048 - accuracy: 0.8515 - val_loss: 0.4504 - val_accuracy: 0.8293\n",
      "Epoch 11/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3969 - accuracy: 0.8547\n",
      "Epoch 11: val_accuracy improved from 0.82928 to 0.83030, saving model to best_LSTM_model.hdf5\n",
      "645/645 [==============================] - 77s 119ms/step - loss: 0.3969 - accuracy: 0.8547 - val_loss: 0.4471 - val_accuracy: 0.8303\n",
      "Epoch 12/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3903 - accuracy: 0.8578\n",
      "Epoch 12: val_accuracy did not improve from 0.83030\n",
      "645/645 [==============================] - 76s 118ms/step - loss: 0.3903 - accuracy: 0.8578 - val_loss: 0.4484 - val_accuracy: 0.8300\n",
      "Epoch 13/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3842 - accuracy: 0.8588\n",
      "Epoch 13: val_accuracy improved from 0.83030 to 0.83496, saving model to best_LSTM_model.hdf5\n",
      "645/645 [==============================] - 76s 117ms/step - loss: 0.3842 - accuracy: 0.8588 - val_loss: 0.4425 - val_accuracy: 0.8350\n",
      "Epoch 14/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3802 - accuracy: 0.8619\n",
      "Epoch 14: val_accuracy did not improve from 0.83496\n",
      "645/645 [==============================] - 76s 118ms/step - loss: 0.3802 - accuracy: 0.8619 - val_loss: 0.4429 - val_accuracy: 0.8334\n",
      "Epoch 15/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3755 - accuracy: 0.8615\n",
      "Epoch 15: val_accuracy did not improve from 0.83496\n",
      "645/645 [==============================] - 76s 117ms/step - loss: 0.3755 - accuracy: 0.8615 - val_loss: 0.4453 - val_accuracy: 0.8316\n",
      "Epoch 16/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3742 - accuracy: 0.8629\n",
      "Epoch 16: val_accuracy did not improve from 0.83496\n",
      "645/645 [==============================] - 76s 118ms/step - loss: 0.3742 - accuracy: 0.8629 - val_loss: 0.4451 - val_accuracy: 0.8335\n",
      "Epoch 17/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3678 - accuracy: 0.8638\n",
      "Epoch 17: val_accuracy did not improve from 0.83496\n",
      "645/645 [==============================] - 76s 118ms/step - loss: 0.3678 - accuracy: 0.8638 - val_loss: 0.4365 - val_accuracy: 0.8348\n",
      "Epoch 18/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3678 - accuracy: 0.8642\n",
      "Epoch 18: val_accuracy improved from 0.83496 to 0.83510, saving model to best_LSTM_model.hdf5\n",
      "645/645 [==============================] - 78s 121ms/step - loss: 0.3678 - accuracy: 0.8642 - val_loss: 0.4396 - val_accuracy: 0.8351\n",
      "Epoch 19/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3638 - accuracy: 0.8667\n",
      "Epoch 19: val_accuracy did not improve from 0.83510\n",
      "645/645 [==============================] - 77s 119ms/step - loss: 0.3638 - accuracy: 0.8667 - val_loss: 0.4347 - val_accuracy: 0.8334\n",
      "Epoch 20/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3626 - accuracy: 0.8687\n",
      "Epoch 20: val_accuracy did not improve from 0.83510\n",
      "645/645 [==============================] - 77s 119ms/step - loss: 0.3626 - accuracy: 0.8687 - val_loss: 0.4395 - val_accuracy: 0.8341\n",
      "Epoch 21/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3574 - accuracy: 0.8690\n",
      "Epoch 21: val_accuracy did not improve from 0.83510\n",
      "645/645 [==============================] - 77s 119ms/step - loss: 0.3574 - accuracy: 0.8690 - val_loss: 0.4353 - val_accuracy: 0.8351\n",
      "Epoch 22/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3599 - accuracy: 0.8685\n",
      "Epoch 22: val_accuracy did not improve from 0.83510\n",
      "645/645 [==============================] - 77s 119ms/step - loss: 0.3599 - accuracy: 0.8685 - val_loss: 0.4369 - val_accuracy: 0.8347\n",
      "Epoch 23/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3529 - accuracy: 0.8715\n",
      "Epoch 23: val_accuracy did not improve from 0.83510\n",
      "645/645 [==============================] - 77s 120ms/step - loss: 0.3529 - accuracy: 0.8715 - val_loss: 0.4404 - val_accuracy: 0.8341\n",
      "Epoch 24/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3500 - accuracy: 0.8730\n",
      "Epoch 24: val_accuracy did not improve from 0.83510\n",
      "645/645 [==============================] - 78s 120ms/step - loss: 0.3500 - accuracy: 0.8730 - val_loss: 0.4350 - val_accuracy: 0.8334\n",
      "Epoch 25/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3509 - accuracy: 0.8727\n",
      "Epoch 25: val_accuracy improved from 0.83510 to 0.83714, saving model to best_LSTM_model.hdf5\n",
      "645/645 [==============================] - 78s 120ms/step - loss: 0.3509 - accuracy: 0.8727 - val_loss: 0.4304 - val_accuracy: 0.8371\n",
      "Epoch 26/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3510 - accuracy: 0.8721\n",
      "Epoch 26: val_accuracy did not improve from 0.83714\n",
      "645/645 [==============================] - 77s 120ms/step - loss: 0.3510 - accuracy: 0.8721 - val_loss: 0.4370 - val_accuracy: 0.8364\n",
      "Epoch 27/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3466 - accuracy: 0.8747\n",
      "Epoch 27: val_accuracy did not improve from 0.83714\n",
      "645/645 [==============================] - 77s 119ms/step - loss: 0.3466 - accuracy: 0.8747 - val_loss: 0.4401 - val_accuracy: 0.8363\n",
      "Epoch 28/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3461 - accuracy: 0.8747\n",
      "Epoch 28: val_accuracy did not improve from 0.83714\n",
      "645/645 [==============================] - 77s 120ms/step - loss: 0.3461 - accuracy: 0.8747 - val_loss: 0.4355 - val_accuracy: 0.8348\n",
      "Epoch 29/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3444 - accuracy: 0.8748\n",
      "Epoch 29: val_accuracy did not improve from 0.83714\n",
      "645/645 [==============================] - 76s 117ms/step - loss: 0.3444 - accuracy: 0.8748 - val_loss: 0.4362 - val_accuracy: 0.8358\n",
      "Epoch 30/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3443 - accuracy: 0.8742\n",
      "Epoch 30: val_accuracy did not improve from 0.83714\n",
      "645/645 [==============================] - 77s 120ms/step - loss: 0.3443 - accuracy: 0.8742 - val_loss: 0.4374 - val_accuracy: 0.8348\n",
      "Epoch 31/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3384 - accuracy: 0.8771\n",
      "Epoch 31: val_accuracy improved from 0.83714 to 0.83758, saving model to best_LSTM_model.hdf5\n",
      "645/645 [==============================] - 77s 120ms/step - loss: 0.3384 - accuracy: 0.8771 - val_loss: 0.4364 - val_accuracy: 0.8376\n",
      "Epoch 32/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3387 - accuracy: 0.8770\n",
      "Epoch 32: val_accuracy improved from 0.83758 to 0.83918, saving model to best_LSTM_model.hdf5\n",
      "645/645 [==============================] - 77s 120ms/step - loss: 0.3387 - accuracy: 0.8770 - val_loss: 0.4351 - val_accuracy: 0.8392\n",
      "Epoch 33/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3384 - accuracy: 0.8775\n",
      "Epoch 33: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 78s 121ms/step - loss: 0.3384 - accuracy: 0.8775 - val_loss: 0.4315 - val_accuracy: 0.8364\n",
      "Epoch 34/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3354 - accuracy: 0.8755\n",
      "Epoch 34: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 77s 120ms/step - loss: 0.3354 - accuracy: 0.8755 - val_loss: 0.4378 - val_accuracy: 0.8364\n",
      "Epoch 35/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3343 - accuracy: 0.8792\n",
      "Epoch 35: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 77s 120ms/step - loss: 0.3343 - accuracy: 0.8792 - val_loss: 0.4378 - val_accuracy: 0.8371\n",
      "Epoch 36/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3326 - accuracy: 0.8797\n",
      "Epoch 36: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 77s 120ms/step - loss: 0.3326 - accuracy: 0.8797 - val_loss: 0.4335 - val_accuracy: 0.8371\n",
      "Epoch 37/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3306 - accuracy: 0.8817\n",
      "Epoch 37: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 77s 120ms/step - loss: 0.3306 - accuracy: 0.8817 - val_loss: 0.4379 - val_accuracy: 0.8377\n",
      "Epoch 38/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3283 - accuracy: 0.8815\n",
      "Epoch 38: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 77s 120ms/step - loss: 0.3283 - accuracy: 0.8815 - val_loss: 0.4354 - val_accuracy: 0.8377\n",
      "Epoch 39/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3285 - accuracy: 0.8819\n",
      "Epoch 39: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 77s 120ms/step - loss: 0.3285 - accuracy: 0.8819 - val_loss: 0.4362 - val_accuracy: 0.8380\n",
      "Epoch 40/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3264 - accuracy: 0.8825\n",
      "Epoch 40: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 77s 120ms/step - loss: 0.3264 - accuracy: 0.8825 - val_loss: 0.4369 - val_accuracy: 0.8374\n",
      "Epoch 41/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3237 - accuracy: 0.8849\n",
      "Epoch 41: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 77s 120ms/step - loss: 0.3237 - accuracy: 0.8849 - val_loss: 0.4428 - val_accuracy: 0.8376\n",
      "Epoch 42/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3234 - accuracy: 0.8824\n",
      "Epoch 42: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 79s 122ms/step - loss: 0.3234 - accuracy: 0.8824 - val_loss: 0.4460 - val_accuracy: 0.8369\n",
      "Epoch 43/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3242 - accuracy: 0.8823\n",
      "Epoch 43: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 77s 120ms/step - loss: 0.3242 - accuracy: 0.8823 - val_loss: 0.4395 - val_accuracy: 0.8380\n",
      "Epoch 44/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3222 - accuracy: 0.8831\n",
      "Epoch 44: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 79s 123ms/step - loss: 0.3222 - accuracy: 0.8831 - val_loss: 0.4425 - val_accuracy: 0.8358\n",
      "Epoch 45/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3198 - accuracy: 0.8841\n",
      "Epoch 45: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 78s 121ms/step - loss: 0.3198 - accuracy: 0.8841 - val_loss: 0.4453 - val_accuracy: 0.8351\n",
      "Epoch 46/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3200 - accuracy: 0.8857\n",
      "Epoch 46: val_accuracy did not improve from 0.83918\n",
      "645/645 [==============================] - 77s 119ms/step - loss: 0.3200 - accuracy: 0.8857 - val_loss: 0.4477 - val_accuracy: 0.8371\n",
      "Epoch 47/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3189 - accuracy: 0.8860\n",
      "Epoch 47: val_accuracy improved from 0.83918 to 0.83976, saving model to best_LSTM_model.hdf5\n",
      "645/645 [==============================] - 77s 119ms/step - loss: 0.3189 - accuracy: 0.8860 - val_loss: 0.4393 - val_accuracy: 0.8398\n",
      "Epoch 48/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3161 - accuracy: 0.8855\n",
      "Epoch 48: val_accuracy did not improve from 0.83976\n",
      "645/645 [==============================] - 78s 121ms/step - loss: 0.3161 - accuracy: 0.8855 - val_loss: 0.4462 - val_accuracy: 0.8366\n",
      "Epoch 49/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3132 - accuracy: 0.8872\n",
      "Epoch 49: val_accuracy did not improve from 0.83976\n",
      "645/645 [==============================] - 77s 120ms/step - loss: 0.3132 - accuracy: 0.8872 - val_loss: 0.4432 - val_accuracy: 0.8369\n",
      "Epoch 50/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3141 - accuracy: 0.8869\n",
      "Epoch 50: val_accuracy did not improve from 0.83976\n",
      "645/645 [==============================] - 77s 120ms/step - loss: 0.3141 - accuracy: 0.8869 - val_loss: 0.4505 - val_accuracy: 0.8376\n",
      "Epoch 51/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3149 - accuracy: 0.8874\n",
      "Epoch 51: val_accuracy did not improve from 0.83976\n",
      "645/645 [==============================] - 77s 120ms/step - loss: 0.3149 - accuracy: 0.8874 - val_loss: 0.4424 - val_accuracy: 0.8385\n",
      "Epoch 52/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3124 - accuracy: 0.8896\n",
      "Epoch 52: val_accuracy did not improve from 0.83976\n",
      "645/645 [==============================] - 78s 120ms/step - loss: 0.3124 - accuracy: 0.8896 - val_loss: 0.4472 - val_accuracy: 0.8374\n",
      "Epoch 53/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3130 - accuracy: 0.8871\n",
      "Epoch 53: val_accuracy did not improve from 0.83976\n",
      "645/645 [==============================] - 77s 119ms/step - loss: 0.3130 - accuracy: 0.8871 - val_loss: 0.4509 - val_accuracy: 0.8389\n",
      "Epoch 54/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3108 - accuracy: 0.8878\n",
      "Epoch 54: val_accuracy did not improve from 0.83976\n",
      "645/645 [==============================] - 77s 119ms/step - loss: 0.3108 - accuracy: 0.8878 - val_loss: 0.4522 - val_accuracy: 0.8382\n",
      "Epoch 55/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3101 - accuracy: 0.8895\n",
      "Epoch 55: val_accuracy did not improve from 0.83976\n",
      "645/645 [==============================] - 76s 118ms/step - loss: 0.3101 - accuracy: 0.8895 - val_loss: 0.4606 - val_accuracy: 0.8358\n",
      "Epoch 56/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3077 - accuracy: 0.8902\n",
      "Epoch 56: val_accuracy improved from 0.83976 to 0.83991, saving model to best_LSTM_model.hdf5\n",
      "645/645 [==============================] - 78s 121ms/step - loss: 0.3077 - accuracy: 0.8902 - val_loss: 0.4475 - val_accuracy: 0.8399\n",
      "Epoch 57/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3101 - accuracy: 0.8891\n",
      "Epoch 57: val_accuracy did not improve from 0.83991\n",
      "645/645 [==============================] - 78s 120ms/step - loss: 0.3101 - accuracy: 0.8891 - val_loss: 0.4434 - val_accuracy: 0.8396\n",
      "Epoch 58/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3056 - accuracy: 0.8916\n",
      "Epoch 58: val_accuracy did not improve from 0.83991\n",
      "645/645 [==============================] - 78s 120ms/step - loss: 0.3056 - accuracy: 0.8916 - val_loss: 0.4487 - val_accuracy: 0.8361\n",
      "Epoch 59/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3091 - accuracy: 0.8894\n",
      "Epoch 59: val_accuracy did not improve from 0.83991\n",
      "645/645 [==============================] - 78s 120ms/step - loss: 0.3091 - accuracy: 0.8894 - val_loss: 0.4540 - val_accuracy: 0.8369\n",
      "Epoch 60/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3070 - accuracy: 0.8913\n",
      "Epoch 60: val_accuracy did not improve from 0.83991\n",
      "645/645 [==============================] - 77s 120ms/step - loss: 0.3070 - accuracy: 0.8913 - val_loss: 0.4484 - val_accuracy: 0.8393\n",
      "Epoch 61/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3027 - accuracy: 0.8915\n",
      "Epoch 61: val_accuracy did not improve from 0.83991\n",
      "645/645 [==============================] - 76s 118ms/step - loss: 0.3027 - accuracy: 0.8915 - val_loss: 0.4511 - val_accuracy: 0.8355\n",
      "Epoch 62/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3038 - accuracy: 0.8907\n",
      "Epoch 62: val_accuracy did not improve from 0.83991\n",
      "645/645 [==============================] - 52s 80ms/step - loss: 0.3038 - accuracy: 0.8907 - val_loss: 0.4545 - val_accuracy: 0.8390\n",
      "Epoch 63/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3011 - accuracy: 0.8935\n",
      "Epoch 63: val_accuracy did not improve from 0.83991\n",
      "645/645 [==============================] - 41s 64ms/step - loss: 0.3011 - accuracy: 0.8935 - val_loss: 0.4510 - val_accuracy: 0.8390\n",
      "Epoch 64/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3011 - accuracy: 0.8904\n",
      "Epoch 64: val_accuracy did not improve from 0.83991\n",
      "645/645 [==============================] - 41s 63ms/step - loss: 0.3011 - accuracy: 0.8904 - val_loss: 0.4506 - val_accuracy: 0.8396\n",
      "Epoch 65/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2993 - accuracy: 0.8915\n",
      "Epoch 65: val_accuracy did not improve from 0.83991\n",
      "645/645 [==============================] - 42s 65ms/step - loss: 0.2993 - accuracy: 0.8915 - val_loss: 0.4554 - val_accuracy: 0.8328\n",
      "Epoch 66/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3025 - accuracy: 0.8934\n",
      "Epoch 66: val_accuracy did not improve from 0.83991\n",
      "645/645 [==============================] - 42s 65ms/step - loss: 0.3025 - accuracy: 0.8934 - val_loss: 0.4518 - val_accuracy: 0.8380\n",
      "Epoch 67/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2989 - accuracy: 0.8919\n",
      "Epoch 67: val_accuracy did not improve from 0.83991\n",
      "645/645 [==============================] - 42s 64ms/step - loss: 0.2989 - accuracy: 0.8919 - val_loss: 0.4580 - val_accuracy: 0.8395\n",
      "Epoch 68/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.3014 - accuracy: 0.8912\n",
      "Epoch 68: val_accuracy improved from 0.83991 to 0.84151, saving model to best_LSTM_model.hdf5\n",
      "645/645 [==============================] - 42s 65ms/step - loss: 0.3014 - accuracy: 0.8912 - val_loss: 0.4531 - val_accuracy: 0.8415\n",
      "Epoch 69/70\n",
      "645/645 [==============================] - ETA: 0s - loss: 0.2983 - accuracy: 0.8918\n",
      "Epoch 69: val_accuracy improved from 0.84151 to 0.84165, saving model to best_LSTM_model.hdf5\n",
      "645/645 [==============================] - 41s 63ms/step - loss: 0.2983 - accuracy: 0.8918 - val_loss: 0.4530 - val_accuracy: 0.8417\n",
      "Epoch 70/70\n",
      "644/645 [============================>.] - ETA: 0s - loss: 0.2963 - accuracy: 0.8945\n",
      "Epoch 70: val_accuracy did not improve from 0.84165\n",
      "645/645 [==============================] - 40s 63ms/step - loss: 0.2964 - accuracy: 0.8945 - val_loss: 0.4524 - val_accuracy: 0.8386\n"
     ]
    }
   ],
   "source": [
    "LSTM_model = Sequential()\n",
    "LSTM_model.add(layers.Embedding(max_words , 20))\n",
    "LSTM_model.add(layers.LSTM(15 , dropout=0.5))\n",
    "LSTM_model.add(layers.Dense(3,activation='softmax'))\n",
    "\n",
    "LSTM_model.compile(optimizer='rmsprop',loss='categorical_crossentropy' , metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"best_LSTM_model.hdf5\",monitor='val_accuracy',verbose=1,save_best_only=True,mode='auto',period=1,save_weights_only=False)\n",
    "history = LSTM_model.fit(X_train , Y_train , epochs=70 , validation_data=(X_test , Y_test) , callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb47c94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215/215 - 5s - loss: 0.4530 - accuracy: 0.8417 - 5s/epoch - 23ms/step\n",
      "Model Accuracy: 0.8416533470153809\n"
     ]
    }
   ],
   "source": [
    "best_model = keras.models.load_model(\"best_LSTM_model.hdf5\")\n",
    "test_loss , test_acc = best_model.evaluate(X_test , Y_test , verbose =2)\n",
    "print(\"Model Accuracy:\" , test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5009e389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215/215 [==============================] - 5s 22ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8de449",
   "metadata": {},
   "source": [
    "The accuracy score of the model is 84% which is very good but it is not a good practice to find out how good your model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "524e8025",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = ['Neutral' , 'Positive' , 'Negative'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "df70888a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 54ms/step\n",
      "Positive\n"
     ]
    }
   ],
   "source": [
    "## Model Testing\n",
    "\n",
    "sequence = tokenizer.texts_to_sequences([\"\"])\n",
    "test = pad_sequences(sequence,maxlen=max_len)\n",
    "print(sentiment[np.around(best_model.predict(test),decimals=0).argmax(axis=1)[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb11217",
   "metadata": {},
   "source": [
    "# Deploying the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33f813f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb7d8b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(best_model , open('LSTM_model.pkl' , 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38240e17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
